import os
import re
from pydub import AudioSegment
import whisper
import unicodedata
import pandas as pd
from jiwer import mer

audio_dir = "/home/zy/task/SPK001-SPK002"
ref_file = "references/text.perconvo"
chunk_length_ms = 30 * 1000

# Text normalization
def normalize_text(text):
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r'[^\w\u4e00-\u9fff]+', ' ', text.lower())
    text = re.sub(r'([\u4e00-\u9fff])', r' \1 ', text)
    return ' '.join(text.split())

# Load ground truth from text perconvo file 
def load_ground_truth_dict(groundtruth_file):
    groundtruth_lines = dict()
    with open(groundtruth_file, "r", encoding="utf-8") as f:
        for line in f:
            if " " in line:  
                audio_id, content = line.strip().split(" ", 1)
                groundtruth_lines[audio_id] = normalize_text(content)
    return groundtruth_lines

# Whisper transcription for each 30s chunks
def transcribe_with_whisper(audio_path, model, chunk_length_ms=30000):
    audio = AudioSegment.from_wav(audio_path)
    chunks = [audio[i:i+chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]
    
    full_transcript = ""
    for i, chunk in enumerate(chunks):
        temp_path = f"temp_chunk_{i}.wav"
        chunk.export(temp_path, format="wav")
        result = model.transcribe(temp_path)
        os.remove(temp_path)

        full_transcript += result["text"].strip() + " "

    return normalize_text(full_transcript)

# Evaluate all audio files in folder
def evaluate_audio_folder(audio_dir, ground_truth_path, model_size="large-v3"):
    model = whisper.load_model(model_size)
    gt_dict = load_ground_truth_dict(ground_truth_path)
    results = []

    for root, _, files in os.walk(audio_dir):
        for filename in files:
            if filename.endswith(".wav"):
                audio_id = filename.replace(".wav", "")
                audio_path = os.path.join(root, filename)

                if audio_id not in gt_dict:
                    print(f"[SKIP] {audio_id} has no ground truth")
                    continue

                print(f"[PROCESS] {audio_id}")
                hypothesis = transcribe_with_whisper(audio_path, model)
                reference = gt_dict[audio_id]
                error_rate = mer(reference, hypothesis)

                results.append({
                    "audio_id": audio_id,
                    "MER": error_rate,
                    "reference": reference,
                    "hypothesis": hypothesis
                })

    # Save the evaluation results as excel file 
    df = pd.DataFrame(results)
    df.to_csv("evaluation_results.csv", index=False)
    print("\nSaved results to evaluation_results.csv")

# Run evaluation for all audio
evaluate_audio_folder(audio_dir, ref_file, model_size="base")
